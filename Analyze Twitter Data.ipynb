{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import requests\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = '6gokUJ7gZ3ixFNkIRUjxL7Xwf'\n",
    "consumer_secret = 'Us8FgUedJ610MDlH4ZK0wga1AtY4wNALQdHf50g3pj7Lm3IWAG'\n",
    "access_token ='542866005-43KxBY08C7knS8VYNAycISB3AyqYmONzt2IhWk3m'\n",
    "access_token_secret = 'YZ8oJWoyR3SgbIfo1I6PJV5wFbMJSVjIukCwiNPryjXzM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonPath = r'C:\\Users\\jame9353\\Documents\\GitHub\\MappingTwitterSentiment\\TwitterJSON'\n",
    "rdfPath = r'C:\\Users\\jame9353\\Documents\\GitHub\\MappingTwitterSentiment\\TwitterRDF'\n",
    "textPath = r'C:\\Users\\jame9353\\Documents\\GitHub\\MappingTwitterSentiment\\TwitterText'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Twitter_SA_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netowlCurl(inFile, outPath, outExtension):\n",
    "    headers = {\n",
    "    'accept': 'application/rdf+xml',\n",
    "    'Authorization': 'netowl ff5e6185-5d63-459b-9765-4ebb905affc8',\n",
    "    }\n",
    "    \n",
    "    \n",
    "    headers['Content-Type'] = 'text/plain'\n",
    "        \n",
    "    params = (\n",
    "        ('language', 'english'),\n",
    "    )\n",
    "    \n",
    "    data = open(inFile, 'rb').read()\n",
    "    response = requests.post('https://api.netowl.com/api/v2/_process', headers=headers, params=params, data=data, verify=False)\n",
    "    r = response.text\n",
    "    outPath = outPath\n",
    "    fileName = os.path.split(d)[1]\n",
    "    if os.path.exists(outPath) == False:\n",
    "        os.mkdir(outPath, mode=0o777,)\n",
    "    outFile = os.path.join(outPath, fileName + outExtension)\n",
    "    open(outFile, \"w\", encoding=\"utf-8\").write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment(tweet_text):\n",
    "    labels = ['positive', 'negative']\n",
    "    testArr = convert_text_to_index_array(tweet_text)\n",
    "    twt = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    twt = pad_sequences(twt, maxlen=86, dtype='int32', padding='post', truncating='post', value=0)\n",
    "    sentiment = model.predict(twt)\n",
    "    accuracy = sentiment[0][np.argmax(sentiment)] * 100\n",
    "    tweetSent = labels[np.argmax(sentiment)]\n",
    "    return tweetSent, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htList = []\n",
    "mentionList = []\n",
    "users = []\n",
    "coords = []\n",
    "replies = []\n",
    "entities = []\n",
    "cColumns = ['Tweet ID', 'Coordinates']\n",
    "rColumns = ['Tweet ID', 'Screen Name', 'Replied To', 'Screen Name']\n",
    "uColumns = ['User', 'Location', 'Description', 'Tweets', 'Date Created', \"Followers\", 'Friends']\n",
    "afrinTweets = []\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#Afrin\",count=100,lang=\"en\").items():\n",
    "    iRow = []\n",
    "    tweetID = tweet.id\n",
    "    date = tweet.created_at\n",
    "    text = tweet.text\n",
    "    user = tweet.user.screen_name\n",
    "    rt = tweet.retweet_count\n",
    "    fav = tweet.favorite_count\n",
    "    location = tweet.user.location\n",
    "    hashtags = tweet.entities.get('hashtags')\n",
    "    hashtag = \"\"\n",
    "    for n in hashtags:\n",
    "        htl = []\n",
    "        hashtag += str(n['text']) + \", \"\n",
    "        htl.append(user)\n",
    "        htl.append(str(n['text']))\n",
    "        htList.append(htl)\n",
    "        \n",
    "    mentions = tweet.entities.get('user_mentions')\n",
    "    mention = \"\"\n",
    "    for n in mentions:\n",
    "        mention += str(n['screen_name']) + \", \"\n",
    "        ml = []\n",
    "        ml.append(user)\n",
    "        ml.append(str(n['screen_name']))\n",
    "        mentionList.append(ml)\n",
    "    tweetSent = \"\"\n",
    "    \n",
    "    iRow.append(tweetID)\n",
    "    iRow.append(date)\n",
    "    iRow.append(text)\n",
    "    iRow.append(user)\n",
    "    iRow.append(rt)\n",
    "    iRow.append(fav)\n",
    "    iRow.append(hashtag)\n",
    "    iRow.append(mention)\n",
    "    iRow.append(tweet.retweeted)\n",
    "    iRow.append(tweetSent)\n",
    "    \n",
    "    afrinTweets.append(iRow)\n",
    "\n",
    "    if user not in users:\n",
    "        uRow = []\n",
    "        uRow.append(user)\n",
    "        uRow.append(location)\n",
    "        uRow.append(tweet.user.description)\n",
    "        uRow.append(tweet.user.statuses_count)\n",
    "        uRow.append(tweet.user.created_at)\n",
    "        uRow.append(tweet.user.followers_count)\n",
    "        uRow.append(tweet.user.friends_count)\n",
    "        users.append(uRow)\n",
    "        \n",
    "    if tweet.geo != None:\n",
    "        cRow = []\n",
    "        coord = tweet.coordinates.get('coordinates')\n",
    "        cRow.append(tweetID)\n",
    "        cRow.append(coord)\n",
    "        coords.append(cRow)\n",
    "        \n",
    "    if tweet.in_reply_to_user_id != None:\n",
    "        rRow = []\n",
    "        replyUID = tweet.in_reply_to_user_id\n",
    "        replySC = tweet.in_reply_to_screen_name\n",
    "        rRow.append(tweetID)\n",
    "        rRow.append(user)\n",
    "        rRow.append(replyUID)\n",
    "        rRow.append(replySC)\n",
    "        replies.append(rRow)\n",
    "        \n",
    "    tj = tweet._json\n",
    "    File = str(tweetID) + \".txt\"\n",
    "    tweetFile = os.path.join(textPath, File)\n",
    "    with open(tweetFile, \"w\", encoding=\"utf-8\") as note:\n",
    "        note.write(text)\n",
    "        note.close()\n",
    "        \n",
    "    json = str(tweetID) + \".json\"\n",
    "    outFile = os.path.join(jsonPath, json)\n",
    "    with open(outFile, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(tj, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Tweet Id', 'Date', 'Text', 'User', 'Retweets', 'Favorites', 'Hashtags', 'Mentions', 'Retweeted', 'Sentiment']\n",
    "df = pd.DataFrame(afrinTweets,columns=columns)\n",
    "usersDF = pd.DataFrame(users, columns=uColumns)\n",
    "repliesDF = pd.DataFrame(replies, columns=rColumns)\n",
    "coordsDF = pd.DataFrame(coords, columns=cColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentList = []\n",
    "for row in df.iterrows():\n",
    "    sl = []\n",
    "    tweetText = row[1]['Text']\n",
    "    tweetID =row[1]['Tweet Id']\n",
    "    user = row[1]['User']\n",
    "    sent = Sentiment(tweetText)\n",
    "    sl.append(tweetID)\n",
    "    sl.append(user)\n",
    "    sl.append(sent[0])\n",
    "    sl.append(sent[1])\n",
    "    sentimentList.append(sl)\n",
    "    \n",
    "columns = ['Tweet Id', 'Handle', 'Sentiment', 'Confidence']\n",
    "sentDF = pd.DataFrame(sentimentList,columns=columns)\n",
    "mask = sentDF.Confidence < 60\n",
    "column_name = 'Sentiment'\n",
    "sentDF.loc[mask, column_name] = \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF['Location'].replace('', np.nan, inplace=True)\n",
    "userLocs = usersDF[['User', 'Location']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDF = df[['Tweet Id', 'Date', 'Text', 'User', 'Retweets', 'Favorites', 'Hashtags', 'Mentions', 'Retweeted']]\n",
    "tweetSent = pd.merge(cleanDF, sentDF, left_on='Tweet Id', right_on='Tweet Id')\n",
    "tweetDF = tweetSent[['Tweet Id', 'Date', 'Text', 'User', 'Retweets', 'Favorites', 'Hashtags', 'Mentions', 'Retweeted', 'Sentiment', 'Confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Walks through the docsPath, identifying files, and appends them to a list.\n",
    "docs = []\n",
    "for root, dirs, files in os.walk(textPath):\n",
    "    for f in files:\n",
    "        filePath = os.path.join(root, f)\n",
    "        docs.append(filePath)\n",
    "        \n",
    "#Iterates though the docs list created previously and \n",
    "# runs the function for each of the documents found. \n",
    "#Passes the function a document derived from the list,\n",
    "# and two variables created in a previous step. \n",
    "\n",
    "for d in docs:\n",
    "    netowlCurl(d, rdfPath, \".rdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from gastrodon import LocalEndpoint, one, QName\n",
    "\n",
    "#Creates a Graph Object that will store all the result of a parse operation \n",
    "# in the next step. \n",
    "g = Graph()\n",
    "\n",
    "#Walks through output path from the netowlCurl function and parses all RDF/XML Documents\n",
    "for root, dir, files in os.walk(rdfPath):\n",
    "    for file in files:\n",
    "        if file.endswith('.rdf'):\n",
    "            filePath = os.path.join(root, file)\n",
    "            print(\"Parsing \" + file + \"...\")\n",
    "            try:\n",
    "                g.parse(filePath, format='xml')\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                \n",
    "#Create Local SPARQL Endpoint on graph created in previous step\n",
    "e = LocalEndpoint(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Uses the SPARQL endpoint to query all of the relationship types and returns the top 10\n",
    "properties=e.select(\"\"\"\n",
    "   SELECT ?p ?type(COUNT(*) AS ?cnt) {\n",
    "      ?s ?p ?o .\n",
    "      ?s rdf:type ?type . \n",
    "   } GROUP BY ?type ORDER BY DESC(?cnt)\n",
    "\"\"\")\n",
    "properties.head(10)\n",
    "    \n",
    "#Writes the full relationship types data frame to a CSV document\n",
    "file_name = 'predicates.csv'\n",
    "properties.to_csv(file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphs the top 10 relationship types\n",
    "with plt.xkcd():\n",
    "    properties.head(10)[\"cnt\"].plot.pie(figsize=(6,6)).set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses the SPARQL endpoint to query all of the relationship types and returns the top 10\n",
    "triplesSel=e.select(\"\"\"\n",
    "   SELECT ?s ?p ?o ?sLabel ?sType ?oLabel ?oType\n",
    "   WHERE{\n",
    "      ?s ?p ?o.\n",
    "      OPTIONAL { ?s rdfs:label ?sLabel }.\n",
    "      OPTIONAL { ?s rdf:type ?sType } .\n",
    "      OPTIONAL { ?o rdfs:label ?oLabel} .\n",
    "      OPTIONAL { ?o rdf:type ?oType} .\n",
    "    }\n",
    "\"\"\")\n",
    "triplesSel.to_csv(tripleList, sep=',', encoding='utf-8')\n",
    "triples = pd.read_csv(tripleList)\n",
    "triples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityQ = triples['sType'] == \"netowl:Entity.Place.City\"\n",
    "label = triples['p'] == \"rdfs:label\"\n",
    "cities = triples[cityQ & label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "import json\n",
    "gis = GIS(\"https://wdcdefense.esri.com/portal\", 'james_jones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_csv = cities.to_csv('Cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_item = gis.content.add({'Type':'CSV',\n",
    "                           'Title':\"Mentioned Cities\"},\n",
    "                           cities_csv)\n",
    "display(csv_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_lyr = csv_item.publish(None, {\"Address\":\"o\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = gis.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.add_layer(csv_lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
