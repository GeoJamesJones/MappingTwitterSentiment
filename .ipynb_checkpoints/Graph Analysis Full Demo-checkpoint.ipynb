{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import visJS2jupyter.visJS_module\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from arcgis.gis import GIS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gis = GIS(\"https://esrifederal.maps.arcgis.com\", 'james_jones_federal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docsPath = r'C:\\Users\\jame9353\\Documents\\GitHub\\Plenary\\Tables'\n",
    "skypeData = 'Skype_Data_1.csv'\n",
    "cleanSkypeCSV = 'Clean_Skype.csv'\n",
    "officeCSV = 'Esri_OfficeLocations.csv'\n",
    "ipCSV = 'IP_Ranges.csv'\n",
    "mk = 'masterKey1.csv'\n",
    "\n",
    "skypeLoc = os.path.join(docsPath, skypeData)\n",
    "cleanSkype = os.path.join(docsPath, cleanSkypeCSV)\n",
    "officeLocs = os.path.join(docsPath, officeCSV)\n",
    "ipRanges = os.path.join(docsPath, ipCSV)\n",
    "masterKey = os.path.join(docsPath, mk)\n",
    "callQuality = os.path.join(docsPath, 'CallQuality.csv')\n",
    "\n",
    "officeDF = pd.read_csv(officeLocs)\n",
    "ipDF = pd.read_csv(ipRanges)\n",
    "mkDF = pd.read_csv(masterKey)\n",
    "cqDF = pd.read_csv(callQuality)\n",
    "\n",
    "ipLoc = pd.merge(ipDF, officeDF, left_on = 'Office', right_on='Office_Code')\n",
    "ipLocs = ipLoc[['Range', 'Office', 'Office_Name', 'X', 'Y']]\n",
    "\n",
    "#Used for determining whether names or anonymous IDs are shown\n",
    "isInternal = True\n",
    "if isInternal == True:\n",
    "    rawDF = pd.read_csv(skypeLoc)\n",
    "    internalCalls = rawDF['internal'] > 0\n",
    "    df = rawDF[internalCalls]\n",
    "    sourceNode = 'jeff_peters@esri.com'\n",
    "    targetNode = 'james_jones@esri.com'\n",
    "    neighbor = 'pmims@esri.com'\n",
    "elif isInternal == False:\n",
    "    rawDF = pd.read_csv(cleanSkype)\n",
    "    internalCalls = rawDF['internal'] > 0\n",
    "    df = rawDF[internalCalls]\n",
    "    sourceNode = 986587\n",
    "    targetNode = 986861\n",
    "    neighbor = 976946\n",
    "    \n",
    "## For \"Examining National Government Call Data\"\n",
    "\n",
    "callItem = gis.content.get('c04d5b1c774e4e9f8fd1b2b659c40717')\n",
    "cqLyr = gis.content.get('50a29c41d9c440c58749e89b05f01e33')\n",
    "jnLyr = gis.content.get('89d4f53c12744a3d9b4e303ca36af244')\n",
    "wdcLyr = gis.content.get('d9d3b139e47944279de60dc3ab75a0b5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esri Office to Office Communication \n",
    "# Internal Call Data Record Analysis (346,000 calls over 6 weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Users Call Data to Examine Call Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersIP = pd.merge(df, ipDF, left_on = 'Caller_Subnet', right_on = 'Range')\n",
    "usersDF = usersIP[['Caller_IP_Address','user_1','Caller_Department','Destination_IP_Address','user_2','Destination_Department', 'internal', 'Office','Caller_Audio_Quality','Destination_Audio_Quality']]\n",
    "users = usersDF.dropna()\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map: Bandwidth Capacity (Office Location) vs. Call Volume (Between Offices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = gis.map(\"Kansas\", zoomlevel=3)\n",
    "map1.basemap = 'gray-vector'\n",
    "items = gis.content.get('2248afaa6a2b45b4b64911cebaabbbd0')\n",
    "mplsLyr = gis.content.get('0f0f88fdda0944feb687d736476c91ff')\n",
    "map1.add_layer(items)\n",
    "map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1.add_layer(mplsLyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Department Call Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIP = pd.merge(df, ipDF, left_on = 'Caller_Subnet', right_on = 'Range')\n",
    "callees = dfIP[['Office', 'Caller_Department','Destination_Department', 'Duration_Seconds']]\n",
    "calleesGroup = callees.groupby(['Caller_Department'])\n",
    "callTotals = calleesGroup.sum()\n",
    "callTotalSort = callTotals.sort_values(by=['Duration_Seconds'], ascending=True).tail(10)\n",
    "\n",
    "officeGroup = callees.groupby(['Office'])\n",
    "officeTotals = officeGroup.sum()\n",
    "officeTotalSort = officeTotals.sort_values(by=['Duration_Seconds'], ascending=False)\n",
    "officeTotalList = []\n",
    "columns = ['Office_Code', 'Call_Duration_Sum']\n",
    "\n",
    "for i in officeTotalSort.index:\n",
    "    iRow = []\n",
    "    iRow.append(i)\n",
    "    iRow.append(officeTotalSort.loc[i][0])\n",
    "    officeTotalList.append(iRow)\n",
    "    \n",
    "officeTotalDF = pd.DataFrame(officeTotalList, columns=columns)\n",
    "\n",
    "#Display a bar chart that shows total call duration per department\n",
    "callTotalPlt = callTotalSort.plot.barh(legend=None, title=\"Total Call Volume per Department\",figsize=(12,8))\n",
    "callTotalPlt.set_ylabel(\"Calling Office\")\n",
    "callTotalPlt.set_xlabel(\"Total Call Duration (seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map: Average Call Quality (Office Location) vs. Call Volume (Between Offices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gis.map(\"Kansas\", zoomlevel=3)\n",
    "m.basemap = 'gray-vector'\n",
    "m.add_layer(items)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_layer(cqLyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Average Audio Quality vs MPLS Bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqDF.plot.scatter(y='Average Audio Quality', x='MPLS Speed',\n",
    "                  title=\"Average Audio Quality vs MPLS Bandwidth\",figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting to Predict Call Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(y='Caller_Audio_Quality', x='Duration_Seconds',\n",
    "                  title=\"Average Audio Quality vs Average Call Duration\",figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlDF = rawDF[['user_1','Caller_Department','user_2','Destination_Department', 'internal','Caller_Audio_Quality','Destination_Audio_Quality', 'Duration_Seconds', 'callee_A_1', 'callee_A_2']]\n",
    "mlDF['Caller_Department'].fillna(\"Unknown\", inplace=True)\n",
    "mlDF['Destination_Department'].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(mlDF, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlDF[\"Caller_Audio_Quality\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = mlDF.corr()\n",
    "corr_matrix[\"Caller_Audio_Quality\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = ['Caller_Audio_Quality', 'Destination_Audio_Quality', 'Duration_Seconds', 'callee_A_1', 'callee_A_2']\n",
    "sm = scatter_matrix(mlDF[attributes], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "dfSub = train_set[['Caller_Audio_Quality', 'Destination_Audio_Quality', 'Duration_Seconds', 'callee_A_1', 'callee_A_2']]\n",
    "imputer.fit(dfSub)\n",
    "train = imputer.transform(dfSub)\n",
    "trainDF = pd.DataFrame(train, columns=dfSub.columns)\n",
    "trainDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "\n",
    "for row in trainDF.iterrows():\n",
    "    if row[1][0] > 4.25 and row[1][1] > 4.25 and row[1][2] >= 60:\n",
    "        label.append(1)\n",
    "    elif row[1][0] >= 3.5 and row[1][0]<= 4.25 and row[1][1] >= 3.5 and row[1][1]<= 4.25 and row[1][2] >= 60: \n",
    "        label.append(2)\n",
    "    elif row[1][0] < 3.5 and row[1][1] < 3.5 and row[1][2] >= 60:\n",
    "        label.append(3)\n",
    "    else:\n",
    "        label.append(4)\n",
    "    \n",
    "trainDF['Label'] = label\n",
    "\n",
    "train_features, train_label = trainDF, trainDF.pop('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(5,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_features.values\n",
    "partial_x_train = train_features.values\n",
    "\n",
    "y_val = train_label.values\n",
    "partial_y_train = train_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy=\"median\")\n",
    "testSub = test_set[['Caller_Audio_Quality', 'Destination_Audio_Quality', 'Duration_Seconds', 'callee_A_1', 'callee_A_2']]\n",
    "imputer.fit(testSub)\n",
    "test = imputer.transform(testSub)\n",
    "testDF = pd.DataFrame(test, columns=testSub.columns)\n",
    "testDF.isnull().sum()\n",
    "tlabel = []\n",
    "\n",
    "for row in testDF.iterrows():\n",
    "    if row[1][0] > 4.25 and row[1][1] > 4.25 and row[1][2] >= 60:\n",
    "        tlabel.append(1)\n",
    "    elif row[1][0] >= 3.5 and row[1][0]<= 4.25 and row[1][1] >= 3.5 and row[1][1]<= 4.25 and row[1][2] >= 60: \n",
    "        tlabel.append(2)\n",
    "    elif row[1][0] < 3.5 and row[1][1] < 3.5 and row[1][2] >= 60:\n",
    "        tlabel.append(3)\n",
    "    else:\n",
    "        tlabel.append(4)\n",
    "    \n",
    "testDF['Label'] = tlabel\n",
    "\n",
    "test_features, test_label = testDF, testDF.pop('Label')\n",
    "\n",
    "x_val_test = test_features.values\n",
    "partial_x_test = test_features.values\n",
    "\n",
    "y_val_test = test_label.values\n",
    "partial_y_test = test_label.values\n",
    "\n",
    "#model = models.Sequential()\n",
    "#model.add(layers.Dense(64, activation='relu', input_shape=(5,)))\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "#model.compile(optimizer='rmsprop',\n",
    "#              loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "model.fit(partial_x_test,\n",
    "          partial_y_test,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val_test, y_val_test))\n",
    "results = model.evaluate(test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_label)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_label) == np.array(test_labels_copy))) / len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('CDR_Values.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('CDR_Values.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedValues = []\n",
    "for i in predictions:\n",
    "    predictedValues.append(np.argmax(i))\n",
    "    \n",
    "testDF['Calculated Value'] = tlabel\n",
    "testDF['Predicted Value'] = predictedValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "for row in testDF.iterrows():\n",
    "    if row[1][5] == row[1][6]:\n",
    "        error.append(\"No Error\")\n",
    "    elif row[1][5] > row[1][6]:\n",
    "        error.append(\"Under-Predict\")\n",
    "    elif row[1][5] < row[1][6]:\n",
    "        error.append(\"Over-Predict\")\n",
    "        \n",
    "testDF['Error'] =error\n",
    "errorDF = testDF.groupby('Error')\n",
    "errorDF[['Predicted Value']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finDF = df[['user_1','Caller_Department','user_2','Destination_Department', 'internal','Caller_Audio_Quality','Destination_Audio_Quality', 'Duration_Seconds', 'callee_A_1', 'callee_A_2']]\n",
    "finDF['Caller_Department'].fillna(\"Unknown\", inplace=True)\n",
    "finDF['Destination_Department'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "totalSub = finDF[['Caller_Audio_Quality', 'Destination_Audio_Quality', 'Duration_Seconds', 'callee_A_1', 'callee_A_2']]\n",
    "imputer.fit(totalSub)\n",
    "total = imputer.transform(totalSub)\n",
    "totalDF = pd.DataFrame(total, columns=totalSub.columns)\n",
    "finalPredictions = model.predict(totalDF)\n",
    "\n",
    "predictedValues = []\n",
    "for i in finalPredictions:\n",
    "    predictedValues.append(np.argmax(i))\n",
    "    \n",
    "totalDF['Predicted Value'] = predictedValues\n",
    "df['Predicted Value'] = predictedValues\n",
    "totalDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- *** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-MPLS Call Volume (Washington, DC Office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isInternal == True:\n",
    "    callerOffice = pd.merge(df, ipDF, left_on = 'Caller_Subnet', right_on = 'Range')\n",
    "    calleeOffice = pd.merge(df, ipDF, left_on = 'Destination_Subnet', right_on = 'Range')\n",
    "    \n",
    "elif isInternal == False:\n",
    "    callerOffice = pd.merge(df, ipDF, left_on = 'Caller_Subnet', right_on = 'Range')\n",
    "    calleeOffice = pd.merge(df, ipDF, left_on = 'Destination_Subnet', right_on = 'Range')\n",
    "    \n",
    "\n",
    "#Generates DataFrames that are used in creating the graphs\n",
    "dept1 = callerOffice['Office'] == 'WDC'\n",
    "dept2 = calleeOffice['Office'] == 'WDC'\n",
    "wdcDF = callerOffice[dept1]\n",
    "wdcDFm = pd.merge(wdcDF, ipDF, left_on = 'Destination_Subnet', right_on = 'Range')\n",
    "wdcCall = calleeOffice[dept2]\n",
    "wdcCallm = pd.merge(wdcCall, ipDF, left_on = 'Destination_Subnet', right_on = 'Range')\n",
    "wdcAll = wdcDFm.append(wdcCallm)\n",
    "wdcCallLoc = pd.merge(wdcAll, officeDF, left_on = 'Office_x', right_on = 'Office_Code')\n",
    "wdcRXLoc = pd.merge(wdcCallLoc, officeDF, left_on = 'Office_y', right_on = 'Office_Code')\n",
    "\n",
    "## For Creating the Graph\n",
    "natGov1 = wdcAll['Caller_Department'] == 'National Govt'\n",
    "natGov2 = wdcAll['Destination_Department'] == 'National Govt'\n",
    "wdcG1 = wdcAll[natGov1]\n",
    "wdcG2 = wdcAll[natGov2]\n",
    "wdcGraph = wdcG1.append(wdcG2)\n",
    "\n",
    "#Creates the necessary graphs to store call data\n",
    "G = nx.Graph()  ## Total call network graph\n",
    "wdcG = nx.Graph()  ## WDC graph\n",
    "ngG = nx.Graph() ## National Government Team Graph\n",
    "\n",
    "nx.from_pandas_dataframe(df, source='user_1', target='user_2', edge_attr='Duration_Seconds', create_using=G)\n",
    "nx.from_pandas_dataframe(wdcAll, source='user_1', target='user_2', edge_attr='Duration_Seconds', create_using=wdcG)\n",
    "nx.from_pandas_dataframe(wdcGraph, source='user_1', target='user_2', edge_attr='Duration_Seconds', create_using=ngG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Locations of National Government Team Members base on Call Records\n",
    "wdcCallees = wdcRXLoc[['user_1','user_2', 'Duration_Seconds','Caller_IP_Address', 'Destination_IP_Address', 'Office_x', 'Office_y','X_y', 'Y_y']]\n",
    "\n",
    "toOfficeGroup = wdcCallees.groupby(['Office_y'])\n",
    "toOfficeTotals = toOfficeGroup.sum()\n",
    "toOfficeTotalSort = toOfficeTotals.sort_values(by=['Duration_Seconds'], ascending=False)\n",
    "\n",
    "toOfficeTotalList = []\n",
    "columns = ['To_Office_Code', 'Call_Duration_Sum']\n",
    "\n",
    "for i in toOfficeTotalSort.index:\n",
    "    iRow = []\n",
    "    iRow.append(i)\n",
    "    iRow.append(toOfficeTotalSort.loc[i][0])\n",
    "    toOfficeTotalList.append(iRow)\n",
    "    \n",
    "toOffDF = pd.DataFrame(toOfficeTotalList, columns=columns)\n",
    "wdcCall = pd.merge(toOffDF, officeDF, left_on = 'To_Office_Code', right_on = 'Office_Code')\n",
    "wdc = wdcCall[['To_Office_Code', 'Call_Duration_Sum', 'Office_Name', 'X', 'Y']]\n",
    "\n",
    "\n",
    "\n",
    "map4 = gis.map(\"Kansas\", zoomlevel=3)\n",
    "map4.basemap = 'gray-vector'\n",
    "\n",
    "map4.add_layer(callItem)\n",
    "\n",
    "map4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map4.add_layer(wdcLyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Call Network\n",
    "\n",
    "The density of the graph indicates out of all possible connections, how many have been made.  The scale is from 0 (no nodes are connected to any other nodes) to 1 (all nodes are connected to all other nodes).  The lower the number the less the total number of connections exist inside of a graph.  A node is an individual entity, and an edge is a connection from one node to another based upon the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = wdcG\n",
    "#graph = ngG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nx.density(graph)\n",
    "info = nx.info(graph)\n",
    "\n",
    "print(\"The graph has a density of \" + str(density))\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Call Network Graph\n",
    "\n",
    "Visualizes the network graph into a link chart. This uses a third party library that is based off of JavaScript D3 visualizations.  The Link Chart is moveable and does provide a pop-up of the entity's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes())\n",
    "edges = list(graph.edges()) \n",
    "\n",
    "# Provides different layout options\n",
    "\n",
    "pos = nx.kamada_kawai_layout(graph)\n",
    "#pos = nx.spring_layout(graph)\n",
    "#pos = nx.circular_layout(graph)\n",
    "\n",
    "nodes_dict = [{\"id\":n,\n",
    "              \"x\":pos[n][0]*1000,\n",
    "              \"y\":pos[n][1]*1000} for n in nodes]\n",
    "\n",
    "node_map = dict(zip(nodes,range(len(nodes))))\n",
    "\n",
    "edges_dict = [{\"source\":node_map[edges[i][0]], \"target\":node_map[edges[i][1]], \n",
    "              \"title\":'test'} for i in range(len(edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visJS2jupyter.visJS_module.visjs_network(nodes_dict,edges_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Bridges\n",
    "A bridge in a graph is an edge whose removal causes the number of connected components of the graph to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridges = list(nx.bridges(graph))\n",
    "print(\"There are a total of \" + str(len(bridges)) + \" bridges in the graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Central Nodes in the Network\n",
    "\n",
    "Degrees represent the total number of nodes that a specific node is connected.\n",
    "\n",
    "Eigenvector centrality computes the centrality for a node based on the centrality of its neighbors. The eigenvector centrality for node i is Ax=λx where A is the adjacency matrix of the graph G with eigenvalue λ. By virtue of the Perron–Frobenius theorem, there is a unique and positive solution if λ is the largest eigenvalue associated with the eigenvector of the adjacency matrix A.\n",
    "        \n",
    "Closeness centrality of a node u is the reciprocal of the average shortest path distance to u over all n-1 reachable nodes. Higher values of closeness indicate higher centrality.\n",
    "    \n",
    "Betweenness centrality of a node v is the sum of the fraction of all-pairs shortest paths that pass through v cB(v)=∑s,t∈Vσ(s,t|v)σ(s,t) where V is the set of nodes, σ(s,t) is the number of shortest (s,t)-paths, and σ(s,t|v) is the number of those paths passing through some node v other than s,t. If s=t, σ(s,t)=1, and if v∈s,t, σ(s,t|v)=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "degree_dict = dict(graph.degree(graph.nodes()))\n",
    "nx.set_node_attributes(graph, degree_dict, 'degree')\n",
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "top_degrees = sorted_degree[:10]\n",
    "for d in top_degrees:\n",
    "    print(\"Name:\", d[0], \"| Degree:\", d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(graph) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(graph) # Run eigenvector centrality\n",
    "closeness_dict = nx.closeness_centrality(graph) # Run eigenvector centrality\n",
    "nx.set_node_attributes(graph, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(graph, eigenvector_dict, 'eigenvector')\n",
    "nx.set_node_attributes(graph, closeness_dict, 'closeness')\n",
    "\n",
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "topBetw = []\n",
    "columns = ['Name', 'Degrees','Betweenness', 'Closeness', 'Eigenvector']\n",
    "for n in sorted_betweenness:\n",
    "    iRow = []\n",
    "    deg = degree_dict[n[0]] # Use degree_dict to access a node's degree\n",
    "    eigen = eigenvector_dict[n[0]] # Use eigenvector_dict to access a node's eigenvector centrality\n",
    "    close = closeness_dict[n[0]] # Use closeness_dict to access a node's closeness centrality\n",
    "    betw = betweenness_dict[n[0]] # Use betweenness_dict to access a node's betweenness centrality\n",
    "    iRow.append(n[0])\n",
    "    iRow.append(deg)\n",
    "    iRow.append(betw)\n",
    "    iRow.append(close)\n",
    "    iRow.append(eigen)\n",
    "    topBetw.append(iRow)\n",
    "betwDF = pd.DataFrame(topBetw, columns=columns)\n",
    "betwCQ = pd.merge(betwDF, mkDF, left_on='Name', right_on='user_1')\n",
    "betwCQDF = betwCQ[['Name', 'Degrees','Betweenness', 'Closeness', 'Eigenvector',\n",
    "                     'Average Call Duration', 'Average Audio Quality']].sort_values('Betweenness',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betwCQDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying total number of microcommunities\n",
    "For each node v, a maximal clique (microcommunity) for v is a largest complete subgraph containing v. The largest maximal clique is sometimes called the maximum clique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumClq = list(nx.find_cliques(graph))\n",
    "print(\"There are \" + str(len(enumClq)) + \" micro-communities in the graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Specific Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows a user to explore as specific node.  Identifies all of the neighbors of a targeted node and returns that as a DataFrame that has been enriched with the number of connections per that node (Degrees), the Betweenness Centrality (Betweenness), Closeness Centrality (Closeness), Eigenvector Centrality (Eigenvector), Average Audio Quality and Average Call Duration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target = sourceNode\n",
    "targetView = nx.degree(graph, target)\n",
    "neighbors = list(nx.all_neighbors(graph, target))\n",
    "neighs = []\n",
    "columns = ['Name', 'Degrees','Betweenness', 'Closeness', 'Eigenvector']\n",
    "for n in neighbors:\n",
    "    iRow = []\n",
    "    deg = degree_dict[n] # Use degree_dict to access a node's degree\n",
    "    eigen = eigenvector_dict[n] # Use eigenvector_dict to access a node's eigenvector centrality\n",
    "    close = closeness_dict[n] # Use closeness_dict to access a node's closeness centrality\n",
    "    betw = betweenness_dict[n] # Use betweenness_dict to access a node's betweenness centrality\n",
    "    iRow.append(n)\n",
    "    iRow.append(deg)\n",
    "    iRow.append(betw)\n",
    "    iRow.append(close)\n",
    "    iRow.append(eigen)\n",
    "    neighs.append(iRow)\n",
    "neighborsDF = pd.DataFrame(neighs, columns=columns)\n",
    "neighborsCQ = pd.merge(neighborsDF, mkDF, left_on='Name', right_on='user_1')\n",
    "nCQDF = neighborsCQ[['Name', 'Degrees','Betweenness', 'Closeness', 'Eigenvector',\n",
    "                     'Average Call Duration', 'Average Audio Quality']].sort_values('Average Audio Quality',ascending=False)\n",
    "print(str(target) + \" is in communication with \" + str(targetView) + \" other individuals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCQDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows a user to identify all common neighbors between two nodes.  The two nodes must be neighbors, meaning that they have an edge connecting them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comNeighs = list(nx.common_neighbors(graph, target, neighbor))\n",
    "comNeighs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain = list(nx.chain_decomposition(graph, target))\n",
    "#chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Connectivity Between Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows a user to identify the shortest path between two nodes.  If the source and target are both specified, return a single list of nodes in a shortest path from the source to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sourceNode in wdcG.nodes() and targetNode in wdcG.nodes():\n",
    "    shortestPath = nx.shortest_path(wdcG, source=targetNode, target=sourceNode)\n",
    "else:\n",
    "    if sourceNode not in wdcG.nodes():\n",
    "        print(sourceNode + \" not in network.\")\n",
    "    elif targetNode not in wdcG.nodes():\n",
    "        print(targetNode + \" not in network.\")\n",
    "print(\"Shortest Path between nodes:  \", shortestPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Microcommunities in the Network to Evaluate Call Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a list of all of the microcommunities that the target node belongs to, limited to microcommunities that have a minimum of three members.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit to top five\n",
    "B = list(nx.find_cliques(G))\n",
    "mcList = []\n",
    "for i in B:\n",
    "    if len(i) > 2:\n",
    "        mcList.append(i)\n",
    "        \n",
    "count = 0\n",
    "microCommunities = []\n",
    "for i in mcList:\n",
    "    if target in i:\n",
    "        for n in i:\n",
    "            if n not in microCommunities:\n",
    "                microCommunities.append(n)\n",
    "        if count <=5:\n",
    "            print(i)\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcDF = df.loc[df['user_1'].isin(microCommunities) & df['user_2'].isin(microCommunities)]\n",
    "mc = mcDF[['user_1','Caller_Subnet','Caller_Department', 'user_2','Destination_Subnet','Destination_Department', 'internal','Caller_Audio_Quality','Destination_Audio_Quality']]\n",
    "mcC = pd.merge(mc, ipLocs, left_on='Caller_Subnet', right_on='Range')\n",
    "mcCS = mcC[['user_1','Caller_Subnet','Caller_Department', 'internal','Caller_Audio_Quality', 'Office', 'X', 'Y']]\n",
    "mcCGroup = mcCS.groupby(['user_1'])\n",
    "mcCGTotals = mcCGroup.mean()\n",
    "mcCGTotalsSort = mcCGTotals.sort_values(by=['Caller_Audio_Quality'], ascending=False)\n",
    "\n",
    "mcCGList = []\n",
    "columns = ['Name', 'Internal Caller', 'Average Audio Quality', 'X', 'Y']\n",
    "\n",
    "for i in mcCGTotalsSort.index:\n",
    "    iRow = []\n",
    "    iRow.append(i)\n",
    "    iRow.append(mcCGTotalsSort.loc[i][0])\n",
    "    iRow.append(mcCGTotalsSort.loc[i][1])\n",
    "    iRow.append(mcCGTotalsSort.loc[i][2])\n",
    "    iRow.append(mcCGTotalsSort.loc[i][3])\n",
    "    mcCGList.append(iRow)\n",
    "    \n",
    "mcCGDF = pd.DataFrame(mcCGList, columns=columns)\n",
    "mcLyr = gis.content.get('1b1d514bdf6c44068b1f43602499fded')\n",
    "clLyr = gis.content.get('949bb386b7ab4e0ab3d48eeece18c2e5')\n",
    "\n",
    "map6 = gis.map(\"Kansas\", zoomlevel=3)\n",
    "map6.basemap = 'gray-vector'\n",
    "map6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map6.add_layer(items)\n",
    "map6.add_layer(clLyr)\n",
    "map6.add_layer(cqLyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
